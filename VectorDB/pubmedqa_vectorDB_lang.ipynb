{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsawant/.conda/envs/faiss/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings.base import Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmedqa_data():\n",
    "    dataset = load_dataset('qiaojin/PubMedQA', 'pqa_artificial')  # Adjust split if needed\n",
    "    print(dataset.keys())\n",
    "    contexts = dataset['train']['context']  # Extract contexts (abstracts)\n",
    "    questions = dataset['train']['question']  # Extract questions\n",
    "    ids = dataset['train']['pubid']  # Extract unique IDs\n",
    "    return contexts, questions, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHuggingFaceEmbeddings(Embeddings):  # Inherit from Embeddings base class\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, \n",
    "                            max_length=512, return_tensors=\"pt\").to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings.cpu().numpy().tolist()  # Convert to list format\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings_vectorDB(input_texts, model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"):\n",
    "    # Initialize the model and tokenizer directly\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to('cuda')\n",
    "    \n",
    "    # Create a custom embeddings clas\n",
    "    \n",
    "    # Process documents in batches\n",
    "    batch_size = 32\n",
    "    documents = []\n",
    "    \n",
    "    # Create Document objects\n",
    "    print(\"Processing documents...\")\n",
    "    for text in tqdm(input_texts):\n",
    "        content = ' '.join(text['contexts'])\n",
    "        documents.append(Document(page_content=content))\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Compute embeddings in batches\n",
    "    print(\"Computing embeddings...\")\n",
    "    texts = [doc.page_content for doc in splits]  # Extract text content\n",
    "    all_embeddings = []\n",
    "    \n",
    "    embeddings = CustomHuggingFaceEmbeddings(model, tokenizer)\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = embeddings.embed_documents(batch_texts)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    dimension = all_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(all_embeddings.astype('float32'))\n",
    "    \n",
    "    # Create and return FAISS vector store\n",
    "    vectorstore = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore={i: doc for i, doc in enumerate(splits)},  # splits are already Document objects\n",
    "        index_to_docstore_id=dict(enumerate(range(len(splits))))\n",
    "    )\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_vectorstore(vectorstore, path=\"./pubmedqa_vectorstore\"):\n",
    "    \"\"\"Save the FAISS vector store\"\"\"\n",
    "    vectorstore.save_local(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "contexts, _, ids = load_pubmedqa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211269/211269 [00:00<00:00, 214230.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22191/22191 [09:36<00:00, 38.49it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorstore = compute_embeddings_vectorDB(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vectorstore(vectorstore, path=\"./pubmedqa_vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
